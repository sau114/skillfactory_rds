{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Компоненты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.3 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "#аугментации изображений\n",
    "!pip install albumentations -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import cv2\n",
    "import albumentations\n",
    "\n",
    "# ML\n",
    "import sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "\n",
    "# DL\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "# from tensorflow.keras.applications import MobileNetV3Large\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "\n",
    "# NLP\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "# plt\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 5\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "print('Python       :', sys.version.split('\\n')[0])\n",
    "print('Numpy        :', np.__version__)\n",
    "print('Tensorflow   :', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# фиксируем рандом\n",
    "RANDOM_SEED = 73\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_file = CatBoostRegressor()\n",
    "\n",
    "# from_file.load_model('../input/car-price-part2-trained-models/model_catboost.cbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import os\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# потому что древний sklearn\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_pred-y_true)/y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# график loss\n",
    "def plot_history(history):\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['MAPE'], label='train')\n",
    "    plt.plot(history.history['val_MAPE'], label='test')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загружаем предобработанные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('../input/car-price-part2-trained-models/train_preprocessed.csv').drop(columns='price')\n",
    "y = pd.read_csv('../input/car-price-part2-trained-models/train_preprocessed.csv', usecols=['price'])\n",
    "\n",
    "X_test = pd.read_csv('../input/car-price-part2-trained-models/test_preprocessed.csv').drop(columns='price')\n",
    "\n",
    "submission = pd.read_csv('../input/sf-dst-car-price-prediction-part2/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.sample(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная обработка для нейронок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# эти признаки сделаем one-hot\n",
    "cat_features_list = ['body_type', \n",
    "                     'brand', \n",
    "                     'color', \n",
    "                     'fuel_type', \n",
    "                     'n_doors', \n",
    "                     'vehicle_transmission', \n",
    "                     'drive_type', \n",
    "                     'n_owners', \n",
    "                     'full_model_name',\n",
    "                     'model_year',\n",
    "                    ]\n",
    "\n",
    "# эти признаки не трогаем\n",
    "bool_features_list = ['is_original_techpass', \n",
    "                      'is_lefthand_drive',\n",
    "                     ]\n",
    "\n",
    "# эти признаки скалируем и нормализуем, если надо\n",
    "num_features_list = ['engine_displacement', \n",
    "                     'engine_power', \n",
    "                     'mileage', \n",
    "                     'production_year',\n",
    "                     'ti_own',\n",
    "                    ]\n",
    "\n",
    "# эти признаки требуют NLP\n",
    "text_features_list = ['description']\n",
    "\n",
    "# этот признак для подгрузки изображения\n",
    "img_features_list = ['sell_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# объединим для корректной обработки\n",
    "data = pd.concat([X.assign(source='train'), \n",
    "                  X_test.assign(source='test'),\n",
    "                 ], axis=0, ignore_index=True)\n",
    "data['description'] = data['description'].fillna('_')  # ннннада\n",
    "print(X.shape, X_test.shape, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_v1(df_input):\n",
    "    df = df_input.copy()\n",
    "    \n",
    "    # приводим тип\n",
    "    for clm in bool_features_list:\n",
    "        df[clm] = df[clm].astype('uint8')\n",
    "    \n",
    "    # нормируем то, что улучшает по результатам исследования\n",
    "    data['engine_power'] = np.log(data['engine_power'] + 1)\n",
    "    data['ti_own'] = np.log(data['ti_own'] + 1.1)\n",
    "#     data['mileage'] = np.log(data['mileage'] + 1)\n",
    "#     data['production_year'] = np.log(np.max(data['production_year']) - data['production_year'] + 1)\n",
    "    # скалируем\n",
    "    scaler = MinMaxScaler()\n",
    "    for clm in num_features_list:\n",
    "        df[clm] = scaler.fit_transform(df[[clm]])[:,0]\n",
    "    # приводим тип\n",
    "    for clm in num_features_list:\n",
    "        df[clm] = df[clm].astype('float32')\n",
    "    \n",
    "    # категориальные one-hot\n",
    "    df = pd.get_dummies(df, columns=cat_features_list)\n",
    "    \n",
    "    # убираем сложные для простой нейронки\n",
    "    df.drop(text_features_list + img_features_list, axis = 1, inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем и проверяем, что получилось\n",
    "data_proc = process_data_v1(data)\n",
    "data_proc.sample(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим обратно\n",
    "X = data_proc.query('source == \"train\"').drop(columns=['source'])\n",
    "X_test = data_proc.query('source == \"test\"').drop(columns=['source'])\n",
    "print(X.shape, X_test.shape, data_proc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Делим на обучение и валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v1 полносвязная"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень простой вариант - просто понять возможности нейронки \"в лоб\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "RANDOM_SEED = 14\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "filename_now = '../working/best_model.hdf5'\n",
    "filename_mlp = '../working/best_nn_1_mlp.hdf5'\n",
    "\n",
    "LR = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_mlp = Sequential([\n",
    "    L.Dense(1024, input_dim=X_train.shape[1], activation=\"relu\"),\n",
    "    L.Dropout(0.5),\n",
    "    L.Dense(256, activation=\"relu\"),\n",
    "    L.Dropout(0.25),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    mdl_mlp,\n",
    "    L.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "mdl_mlp.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LR, amsgrad=False,)\n",
    "\n",
    "model.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filename_now,\n",
    "                             monitor='val_MAPE', \n",
    "                             verbose=1, \n",
    "                             mode='min',\n",
    "                             save_best_only = True,\n",
    "                            )\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_MAPE',\n",
    "                          patience=20,\n",
    "                          min_delta = 0.001,\n",
    "                          restore_best_weights=True,\n",
    "                          verbose=1,\n",
    "                         )\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(filename_mlp)\n",
    "except:\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        batch_size=128,\n",
    "                        epochs=1000,    # wait EarlyStopping\n",
    "                        validation_data=(X_valid, y_valid),\n",
    "                        callbacks=callbacks_list,\n",
    "                        verbose=0,\n",
    "                       )\n",
    "    plot_history(history)\n",
    "    model.load_weights(filename_now)\n",
    "    model.save(filename_mlp)\n",
    "\n",
    "valid_predict = model.predict(X_valid)\n",
    "print(f\"TEST mape: {(mape(y_valid, valid_predict))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Каггл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['price'] = model.predict(X_test)\n",
    "submission.to_csv('submission_nn_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v2 ... + NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У меня получился хороший результат через emedding с использованием doc2vec (чуть лучше, чем word2vec с усреднением). Плюс само обучение происходит проще. Результат добавим к полносвязной нейронке из предыдущего варианта."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Готовим embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# простая токенизация, т.к. текст уже подготовлен\n",
    "tkn_description = data['description'].str.split()\n",
    "\n",
    "# корпусы для doc2vec\n",
    "C_train = [doc2vec.TaggedDocument(tkn_description.iloc[i], [i]) for i in X_train.index]\n",
    "C_valid = [doc2vec.TaggedDocument(tkn_description.iloc[i], [i]) for i in X_valid.index]\n",
    "C_test = [tkn_description.iloc[i] for i in X_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сама модель\n",
    "mdl_d2v = doc2vec.Doc2Vec(vector_size=EMBEDDING_SIZE,\n",
    "                          min_count=2,  # встречается хотя бы 2 раза\n",
    "                          seed=RANDOM_SEED,\n",
    "                         )\n",
    "# строим словарь\n",
    "mdl_d2v.build_vocab(C_train)\n",
    "# и учим\n",
    "mdl_d2v.train(C_train, \n",
    "              total_examples=mdl_d2v.corpus_count, \n",
    "              epochs=mdl_d2v.epochs\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# обработаем все тексты\n",
    "emb_description = tkn_description.map(mdl_d2v.infer_vector)\n",
    "emb_description.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_train = np.array(emb_description.iloc[X_train.index].tolist())\n",
    "V_valid = np.array(emb_description.iloc[X_valid.index].tolist())\n",
    "V_test = np.array(emb_description.iloc[X_test.index].tolist())\n",
    "print(V_train.shape)\n",
    "print(V_valid.shape)\n",
    "print(V_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем multi-input NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 55\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "filename_nlp = '../working/best_nn_2_d2v.hdf5'\n",
    "\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_nlp = Sequential([\n",
    "    L.Dense(1024, input_dim=V_train.shape[1], activation=\"relu\"),\n",
    "    L.Dropout(0.25),\n",
    "    L.Dense(256, activation=\"relu\"),\n",
    "    L.Dropout(0.25),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = L.concatenate([mdl_nlp.output, mdl_mlp.output])\n",
    "\n",
    "# being our regression head\n",
    "head = L.Dense(128, activation=\"relu\")(combinedInput)\n",
    "head = L.Dense(1, activation=\"linear\")(head)\n",
    "\n",
    "model = Model(inputs=[mdl_nlp.input, mdl_mlp.input], outputs=head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LR, amsgrad=False,)\n",
    "\n",
    "model.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filename_now,\n",
    "                             monitor='val_MAPE', \n",
    "                             verbose=1, \n",
    "                             mode='min',\n",
    "                             save_best_only = True,\n",
    "                            )\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_MAPE',\n",
    "                          patience=20,\n",
    "                          min_delta = 0.001,\n",
    "                          restore_best_weights=True,\n",
    "                          verbose=1,\n",
    "                         )\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(filename_nlp)\n",
    "except:\n",
    "    history = model.fit([V_train, X_train], y_train,\n",
    "                         batch_size=128,\n",
    "                         epochs=1000, # until EarlyStopping\n",
    "                         validation_data=([V_valid, X_valid], y_valid),\n",
    "                         callbacks=callbacks_list,\n",
    "                         verbose=0,\n",
    "                        )\n",
    "    plot_history(history)\n",
    "    model.load_weights(filename_now)\n",
    "    model.save(filename_nlp)\n",
    "\n",
    "valid_predict = model.predict([V_valid, X_valid])\n",
    "print(f\"TEST mape: {(mape(y_valid, valid_predict))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Каггл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['price'] = model.predict([V_test, X_test])\n",
    "submission.to_csv('submission_nn_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# v3 ... + Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# убедимся, что цены и фото подгрузились верно\n",
    "plt.figure(figsize = (12,8))\n",
    "\n",
    "random_image = X_train.head(9)\n",
    "random_image_paths = data.loc[random_image.index, 'sell_id'].values\n",
    "random_image_cat = y.loc[random_image.index, 'price'].values\n",
    "\n",
    "for index, path in enumerate(random_image_paths):\n",
    "    im = PIL.Image.open('../input/sf-dst-car-price-prediction-part2/img/img/' + str(path) + '.jpg')\n",
    "    plt.subplot(3, 3, index + 1)\n",
    "    plt.imshow(im)\n",
    "    plt.title('price: ' + str(random_image_cat[index]))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у нас чистые изображения одного размера\n",
    "IMG_SIZE = (320, 240)\n",
    "\n",
    "def get_image_array(index):\n",
    "    images = []\n",
    "    for index, sell_id in enumerate(data.loc[index, 'sell_id'].values):\n",
    "        image = cv2.imread('../input/sf-dst-car-price-prediction-part2/img/img/' + str(sell_id) + '.jpg')\n",
    "        assert(image is not None)\n",
    "        image = cv2.resize(image, IMG_SIZE)  # без этого возникают траблы\n",
    "        images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) # перевёрнем цветовую схему, чтобы не бесило\n",
    "    images = np.array(images)\n",
    "    print('images shape', images.shape, 'dtype', images.dtype)\n",
    "    return(images)\n",
    "\n",
    "# все изображения в RAM, можем себе позволить\n",
    "I_train = get_image_array(X_train.index)\n",
    "I_valid = get_image_array(X_valid.index)\n",
    "I_test = get_image_array(X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим правильные цвета\n",
    "plt.figure(figsize = (12,8))\n",
    "for i in range(9):\n",
    "    img = I_train[i]\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Images in RAM: {(I_train.nbytes+I_valid.nbytes+I_test.nbytes)/1024**3:0.2f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    HorizontalFlip, \n",
    "    MultiplicativeNoise, GaussNoise, JpegCompression,\n",
    "    MotionBlur, MedianBlur, Blur,\n",
    "    ShiftScaleRotate,\n",
    "    OpticalDistortion, GridDistortion, ElasticTransform,\n",
    "    CLAHE, IAASharpen, IAAEmboss, RandomBrightnessContrast,\n",
    "    HueSaturationValue,\n",
    "    OneOf, Compose,\n",
    ")\n",
    "\n",
    "# документация здесь https://albumentations.ai/docs/getting_started/image_augmentation/\n",
    "# default p=0.5\n",
    "augmentation = Compose([\n",
    "    HorizontalFlip(),\n",
    "    OneOf([\n",
    "        MultiplicativeNoise(),\n",
    "        GaussNoise(),\n",
    "        JpegCompression(),\n",
    "    ]),\n",
    "    OneOf([\n",
    "        MotionBlur(),\n",
    "        MedianBlur(),\n",
    "        Blur(),\n",
    "    ]),\n",
    "    ShiftScaleRotate(rotate_limit=20),\n",
    "#     OneOf([\n",
    "#         OpticalDistortion(),\n",
    "#         GridDistortion(),\n",
    "#         ElasticTransform(),\n",
    "#     ]),\n",
    "    OneOf([\n",
    "        CLAHE(),\n",
    "        IAASharpen(),\n",
    "        IAAEmboss (),\n",
    "        RandomBrightnessContrast(),\n",
    "    ]),\n",
    "    HueSaturationValue(),\n",
    "], p=1)\n",
    "\n",
    "# пример\n",
    "plt.figure(figsize = (12,8))\n",
    "for i in range(9):\n",
    "    img = augmentation(image = I_train[0])['image']\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_augmentations(images):\n",
    "#   print('применение аугментаций', end = '')\n",
    "#   augmented_images = np.empty(images.shape)\n",
    "#   for i in range(images.shape[0]):\n",
    "#     if i % 200 == 0:\n",
    "#       print('.', end = '')\n",
    "#     augment_dict = augmentation(image = images[i])\n",
    "#     augmented_image = augment_dict['image']\n",
    "#     augmented_images[i] = augmented_image\n",
    "#   print('')\n",
    "#   return augmented_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.data.Dataset\n",
    "Если все изображения мы будем хранить в памяти, то может возникнуть проблема ее нехватки. Не храните все изображения в памяти целиком!\n",
    "\n",
    "Метод .fit() модели keras может принимать либо данные в виде массивов или тензоров, либо разного рода итераторы, из которых наиболее современным и гибким является [tf.data.Dataset](https://www.tensorflow.org/guide/data). Он представляет собой конвейер, то есть мы указываем, откуда берем данные и какую цепочку преобразований с ними выполняем. Далее мы будем работать с tf.data.Dataset.\n",
    "\n",
    "Dataset хранит информацию о конечном или бесконечном наборе кортежей (tuple) с данными и может возвращать эти наборы по очереди. Например, данными могут быть пары (input, target) для обучения нейросети. С данными можно осуществлять преобразования, которые осуществляются по мере необходимости ([lazy evaluation](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BD%D0%B8%D0%B2%D1%8B%D0%B5_%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F)).\n",
    "\n",
    "`tf.data.Dataset.from_tensor_slices(data)` - создает датасет из данных, которые представляют собой либо массив, либо кортеж из массивов. Деление осуществляется по первому индексу каждого массива. Например, если `data = (np.zeros((128, 256, 256)), np.zeros(128))`, то датасет будет содержать 128 элементов, каждый из которых содержит один массив 256x256 и одно число.\n",
    "\n",
    "`dataset2 = dataset1.map(func)` - применение функции к датасету; функция должна принимать столько аргументов, каков размер кортежа в датасете 1 и возвращать столько, сколько нужно иметь в датасете 2. Пусть, например, датасет содержит изображения и метки, а нам нужно создать датасет только из изображений, тогда мы напишем так: `dataset2 = dataset.map(lambda img, label: img)`.\n",
    "\n",
    "`dataset2 = dataset1.batch(8)` - группировка по батчам; если датасет 2 должен вернуть один элемент, то он берет из датасета 1 восемь элементов, склеивает их (нулевой индекс результата - номер элемента) и возвращает.\n",
    "\n",
    "`dataset.__iter__()` - превращение датасета в итератор, из которого можно получать элементы методом `.__next__()`. Итератор, в отличие от самого датасета, хранит позицию текущего элемента. Можно также перебирать датасет циклом for.\n",
    "\n",
    "`dataset2 = dataset1.repeat(X)` - датасет 2 будет повторять датасет 1 X раз.\n",
    "\n",
    "Если нам нужно взять из датасета 1000 элементов и использовать их как тестовые, а остальные как обучающие, то мы напишем так:\n",
    "\n",
    "`test_dataset = dataset.take(1000)\n",
    "train_dataset = dataset.skip(1000)`\n",
    "\n",
    "Датасет по сути неизменен: такие операции, как map, batch, repeat, take, skip никак не затрагивают оригинальный датасет. Если датасет хранит элементы [1, 2, 3], то выполнив 3 раза подряд функцию dataset.take(1) мы получим 3 новых датасета, каждый из которых вернет число 1. Если же мы выполним функцию dataset.skip(1), мы получим датасет, возвращающий числа [2, 3], но исходный датасет все равно будет возвращать [1, 2, 3] каждый раз, когда мы его перебираем.\n",
    "\n",
    "tf.Dataset всегда выполняется в graph-режиме (в противоположность eager-режиму), поэтому либо преобразования (`.map()`) должны содержать только tensorflow-функции, либо мы должны использовать tf.py_function в качестве обертки для функций, вызываемых в `.map()`. Подробнее можно прочитать [здесь](https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP part\n",
    "# tokenize = Tokenizer(num_words=MAX_WORDS)\n",
    "# tokenize.fit_on_texts(data.description)\n",
    "\n",
    "# def tokenize_(descriptions):\n",
    "#   return sequence.pad_sequences(tokenize.texts_to_sequences(descriptions), maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     return tokenize_([text.numpy().decode('utf-8')])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    return augmentation(image = image.numpy())['image']\n",
    "\n",
    "def tf_process_train_dataset_element(image, table_data, text_emb, price):\n",
    "    im_shape = image.shape\n",
    "    [image,] = tf.py_function(process_image, [image], [tf.uint8])\n",
    "    image.set_shape(im_shape)\n",
    "#     [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n",
    "    return (image, table_data, text_emb), price\n",
    "\n",
    "def tf_process_val_dataset_element(image, table_data, text_emb, price):\n",
    "#     [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n",
    "    return (image, table_data, text_emb), price\n",
    "\n",
    "D_train = tf.data.Dataset.from_tensor_slices((\n",
    "    I_train, \n",
    "    X_train, \n",
    "    V_train, \n",
    "    y_train,\n",
    "    )).map(tf_process_train_dataset_element)\n",
    "\n",
    "D_valid = tf.data.Dataset.from_tensor_slices((\n",
    "    I_valid,\n",
    "    X_valid,\n",
    "    V_valid,\n",
    "    y_valid,\n",
    "    )).map(tf_process_val_dataset_element)\n",
    "\n",
    "y_test = np.zeros(len(X_test))\n",
    "\n",
    "D_test = tf.data.Dataset.from_tensor_slices((\n",
    "    I_test, \n",
    "    X_test,\n",
    "    V_test,\n",
    "    y_test,\n",
    "    )).map(tf_process_val_dataset_element)\n",
    "\n",
    "#проверяем, что нет ошибок (не будет выброшено исключение):\n",
    "next(iter(D_train));\n",
    "next(iter(D_valid));\n",
    "next(iter(D_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 16\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "filename_cnn = '../working/best_nn_3_cnn.hdf5'\n",
    "\n",
    "LR = 1e-2  # используем callback для управления уменьшением\n",
    "BATCH_SIZE = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удалим файл модели, если хотим обучить заново\n",
    "# os.remove(filename_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# efficientnet_model = tf.keras.applications.efficientnet.EfficientNetB3(weights = 'imagenet', include_top = False, input_shape = (size[1], size[0], 3))\n",
    "# efficientnet_output = L.GlobalAveragePooling2D()(efficientnet_model.output)\n",
    "\n",
    "mdl_cnn = EfficientNetB3(\n",
    "    input_shape = (IMG_SIZE[1], IMG_SIZE[0], 3),\n",
    "    include_top = False,\n",
    "    weights = 'imagenet',\n",
    "    pooling = 'avg',    # avg - max - None\n",
    "    classifier_activation = 'softmax',    # softmax - None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# объединяем выходы трех нейросетей\n",
    "combinedInput = L.concatenate([mdl_cnn.output, mdl_mlp.output, mdl_nlp.output])\n",
    "\n",
    "# being our regression head\n",
    "head = L.Dense(256, activation=\"relu\")(combinedInput)\n",
    "head = L.Dense(1,)(head)\n",
    "\n",
    "model = Model(inputs=[mdl_cnn.input, mdl_mlp.input, mdl_nlp.input], outputs=head)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(LR, amsgrad=False,)\n",
    "\n",
    "model.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filename_now,\n",
    "                             monitor='val_MAPE', \n",
    "                             verbose=1, \n",
    "                             mode='min',\n",
    "                             save_best_only = True,\n",
    "                            )\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_MAPE',\n",
    "                          patience=25,\n",
    "                          min_delta = 0.001,\n",
    "                          restore_best_weights=True,\n",
    "                          verbose=1,\n",
    "                         )\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_MAPE',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    mode='min',\n",
    "    min_delta=0.001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "callbacks_list = [checkpoint, earlystop, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(filename_cnn)\n",
    "except:\n",
    "    history = model.fit(D_train.batch(BATCH_SIZE),\n",
    "#                          batch_size=128,\n",
    "                         epochs=1000, # until EarlyStopping\n",
    "                         validation_data=D_valid.batch(BATCH_SIZE),\n",
    "                         callbacks=callbacks_list,\n",
    "                         verbose=0,\n",
    "                        )\n",
    "    plot_history(history)\n",
    "    model.load_weights(filename_now)\n",
    "    model.save(filename_cnn)\n",
    "\n",
    "valid_predict = model.predict(D_valid.batch(BATCH_SIZE))\n",
    "print(f\"TEST mape: {(mape(y_valid, valid_predict))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кагл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['price'] = model.predict(D_test.batch(BATCH_SIZE))\n",
    "submission.to_csv('submission_nn_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "# history = model.fit(D_train.batch(30),\n",
    "#                     epochs=100,\n",
    "#                     validation_data = D_valid.batch(30),\n",
    "#                     callbacks=callbacks_list\n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.title('Loss')\n",
    "# plt.plot(history.history['MAPE'], label='train')\n",
    "# plt.plot(history.history['val_MAPE'], label='test')\n",
    "# plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('../working/best_model.hdf5')\n",
    "# model.save('../working/nn_final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predict_nn3 = model.predict(D_valid.batch(30))\n",
    "# print(f\"TEST mape: {(mape(y_test, test_predict_nn3[:,0]))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub_predict_nn3 = model.predict(sub_dataset.batch(30))\n",
    "# sample_submission['price'] = sub_predict_nn3[:,0]\n",
    "# sample_submission.to_csv('nn3_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Общие рекомендации:\n",
    "* Попробовать разные архитектуры\n",
    "* Провести более детальный анализ результатов\n",
    "* Попробовать различные подходы в управление LR и оптимизаторы\n",
    "* Поработать с таргетом\n",
    "* Использовать Fine-tuning\n",
    "\n",
    "#### Tabular\n",
    "* В нейросеть желательно подавать данные с распределением, близким к нормальному, поэтому от некоторых числовых признаков имеет смысл взять логарифм перед нормализацией. Пример:\n",
    "`modelDateNorm = np.log(2020 - data['modelDate'])`\n",
    "Статья по теме: https://habr.com/ru/company/ods/blog/325422\n",
    "\n",
    "* Извлечение числовых значений из текста:\n",
    "Парсинг признаков 'engineDisplacement', 'enginePower', 'Владение' для извлечения числовых значений.\n",
    "\n",
    "* Cокращение размерности категориальных признаков\n",
    "Признак name 'name' содержит данные, которые уже есть в других столбцах ('enginePower', 'engineDisplacement', 'vehicleTransmission'). Можно удалить эти данные. Затем можно еще сильнее сократить размерность, например выделив наличие xDrive в качестве отдельного признака.\n",
    "\n",
    "* Поработать над Feature engineering\n",
    "\n",
    "\n",
    "\n",
    "#### NLP\n",
    "* Выделить из описаний часто встречающиеся блоки текста, заменив их на кодовые слова или удалив\n",
    "* Сделать предобработку текста, например сделать лемматизацию - алгоритм ставящий все слова в форму по умолчанию (глаголы в инфинитив и т. д.), чтобы токенайзер не преобразовывал разные формы слова в разные числа\n",
    "Статья по теме: https://habr.com/ru/company/Voximplant/blog/446738/\n",
    "* Поработать над алгоритмами очистки и аугментации текста\n",
    "\n",
    "\n",
    "\n",
    "#### CV\n",
    "* Попробовать различные аугментации\n",
    "* Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_predict = (test_predict_catboost + test_predict_nn3[:,0]) / 2\n",
    "print(f\"TEST mape: {(mape(y_test, blend_predict))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blend_sub_predict = (sub_predict_catboost + sub_predict_nn3[:,0]) / 2\n",
    "sample_submission['price'] = blend_sub_predict\n",
    "sample_submission.to_csv('blend_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Bonus: проброс признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "model_mlp = Sequential()\n",
    "model_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model_mlp.add(L.Dropout(0.5))\n",
    "model_mlp.add(L.Dense(256, activation=\"relu\"))\n",
    "model_mlp.add(L.Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE Input\n",
    "# Iput\n",
    "productiondate = L.Input(shape=[1], name=\"productiondate\")\n",
    "# Embeddings layers\n",
    "emb_productiondate = L.Embedding(len(X.productionDate.unique().tolist())+1, 20)(productiondate)\n",
    "f_productiondate = L.Flatten()(emb_productiondate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinedInput = L.concatenate([model_mlp.output, f_productiondate,])\n",
    "# being our regression head\n",
    "head = L.Dense(64, activation=\"relu\")(combinedInput)\n",
    "head = L.Dense(1, activation=\"linear\")(head)\n",
    "\n",
    "model = Model(inputs=[model_mlp.input, productiondate], outputs=head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "model.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train, X_train.productionDate.values], y_train,\n",
    "                    batch_size=512,\n",
    "                    epochs=500, # фактически мы обучаем пока EarlyStopping не остановит обучение\n",
    "                    validation_data=([X_test, X_test.productionDate.values], y_test),\n",
    "                    callbacks=callbacks_list\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../working/best_model.hdf5')\n",
    "test_predict_nn_bonus = model.predict([X_test, X_test.productionDate.values])\n",
    "print(f\"TEST mape: {(mape(y_test, test_predict_nn_bonus[:,0]))*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
