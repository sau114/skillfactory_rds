{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Библиотеки","metadata":{}},{"cell_type":"markdown","source":"## Компоненты","metadata":{}},{"cell_type":"code","source":"!python -m pip install --upgrade pip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install tensorflow==2.3 -q","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#аугментации изображений\n!pip install albumentations -q","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Импорт","metadata":{}},{"cell_type":"code","source":"import sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport PIL\nimport cv2\nimport albumentations\n\n# ML\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom catboost import Pool, CatBoostRegressor\n\n# DL\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing import sequence\n# from tensorflow.keras.applications import MobileNetV3Large\nfrom tensorflow.keras.applications import EfficientNetB3\n\n# NLP\nfrom gensim.models import doc2vec\n\n# plt\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\n%config InlineBackend.figure_format = 'svg' \n%matplotlib inline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Python       :', sys.version.split('\\n')[0])\nprint('Numpy        :', np.__version__)\nprint('Tensorflow   :', tf.__version__)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# фиксируем рандом\nRANDOM_SEED = 73\nnp.random.seed(RANDOM_SEED)","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from_file = CatBoostRegressor()\n\n# from_file.load_model('../input/car-price-part2-trained-models/model_catboost.cbm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import random\n# import os\n# import re","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip freeze > requirements.txt","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Функции","metadata":{}},{"cell_type":"code","source":"# потому что древний sklearn\ndef mape(y_true, y_pred):\n    return np.mean(np.abs((y_pred-y_true)/y_true))","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# график loss\ndef plot_history(history):\n    plt.title('Loss')\n    plt.plot(history.history['MAPE'], label='train')\n    plt.plot(history.history['val_MAPE'], label='test')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Данные","metadata":{}},{"cell_type":"markdown","source":"## Загружаем предобработанные","metadata":{}},{"cell_type":"code","source":"X = pd.read_csv('../input/car-price-part2-trained-models/train_preprocessed.csv').drop(columns='price')\ny = pd.read_csv('../input/car-price-part2-trained-models/train_preprocessed.csv', usecols=['price'])\n\nX_test = pd.read_csv('../input/car-price-part2-trained-models/test_preprocessed.csv').drop(columns='price')\n\nsubmission = pd.read_csv('../input/sf-dst-car-price-prediction-part2/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.sample(1).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Дополнительная обработка для нейронок","metadata":{}},{"cell_type":"code","source":"# эти признаки сделаем one-hot\ncat_features_list = ['body_type', \n                     'brand', \n                     'color', \n                     'fuel_type', \n                     'n_doors', \n                     'vehicle_transmission', \n                     'drive_type', \n                     'n_owners', \n                     'full_model_name',\n                     'model_year',\n                    ]\n\n# эти признаки не трогаем\nbool_features_list = ['is_original_techpass', \n                      'is_lefthand_drive',\n                     ]\n\n# эти признаки скалируем и нормализуем, если надо\nnum_features_list = ['engine_displacement', \n                     'engine_power', \n                     'mileage', \n                     'production_year',\n                     'ti_own',\n                    ]\n\n# эти признаки требуют NLP\ntext_features_list = ['description']\n\n# этот признак для подгрузки изображения\nimg_features_list = ['sell_id']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# объединим для корректной обработки\ndata = pd.concat([X.assign(source='train'), \n                  X_test.assign(source='test'),\n                 ], axis=0, ignore_index=True)\ndata['description'] = data['description'].fillna('_')  # ннннада\nprint(X.shape, X_test.shape, data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_data_v1(df_input):\n    df = df_input.copy()\n    \n    # приводим тип\n    for clm in bool_features_list:\n        df[clm] = df[clm].astype('uint8')\n    \n    # нормируем то, что улучшает по результатам исследования\n    data['engine_power'] = np.log(data['engine_power'] + 1)\n    data['ti_own'] = np.log(data['ti_own'] + 1.1)\n#     data['mileage'] = np.log(data['mileage'] + 1)\n#     data['production_year'] = np.log(np.max(data['production_year']) - data['production_year'] + 1)\n    # скалируем\n    scaler = MinMaxScaler()\n    for clm in num_features_list:\n        df[clm] = scaler.fit_transform(df[[clm]])[:,0]\n    # приводим тип\n    for clm in num_features_list:\n        df[clm] = df[clm].astype('float32')\n    \n    # категориальные one-hot\n    df = pd.get_dummies(df, columns=cat_features_list)\n    \n    # убираем сложные для простой нейронки\n    df.drop(text_features_list + img_features_list, axis = 1, inplace=True)\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Запускаем и проверяем, что получилось\ndata_proc = process_data_v1(data)\ndata_proc.sample(3).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_proc.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Разделим обратно\nX = data_proc.query('source == \"train\"').drop(columns=['source'])\nX_test = data_proc.query('source == \"test\"').drop(columns=['source'])\nprint(X.shape, X_test.shape, data_proc.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Делим на обучение и валидацию","metadata":{}},{"cell_type":"code","source":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\nprint(X_train.shape, y_train.shape)\nprint(X_valid.shape, y_valid.shape)\nprint(X_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# v1 полносвязная","metadata":{}},{"cell_type":"markdown","source":"Очень простой вариант - просто понять возможности нейронки \"в лоб\".","metadata":{}},{"cell_type":"markdown","source":"## Подготовка","metadata":{}},{"cell_type":"code","source":"tf.keras.backend.clear_session()\n\nRANDOM_SEED = 14\nnp.random.seed(RANDOM_SEED)\n\nfilename_now = '../working/best_model.hdf5'\nfilename_mlp = '../working/best_nn_1_mlp.hdf5'\n\nLR = 1e-2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mdl_mlp = Sequential([\n    L.Dense(1024, input_dim=X_train.shape[1], activation=\"relu\"),\n    L.Dropout(0.5),\n    L.Dense(256, activation=\"relu\"),\n    L.Dropout(0.25),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential([\n    mdl_mlp,\n    L.Dense(1, activation=\"linear\"),\n])\nmdl_mlp.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(LR, amsgrad=False,)\n\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(filename_now,\n                             monitor='val_MAPE', \n                             verbose=1, \n                             mode='min',\n                             save_best_only = True,\n                            )\n\nearlystop = EarlyStopping(monitor='val_MAPE',\n                          patience=20,\n                          min_delta = 0.001,\n                          restore_best_weights=True,\n                          verbose=1,\n                         )\n\ncallbacks_list = [checkpoint, earlystop]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Обучение","metadata":{}},{"cell_type":"code","source":"try:\n    model.load_weights(filename_mlp)\nexcept:\n    history = model.fit(X_train, y_train,\n                        batch_size=128,\n                        epochs=1000,    # wait EarlyStopping\n                        validation_data=(X_valid, y_valid),\n                        callbacks=callbacks_list,\n                        verbose=0,\n                       )\n    plot_history(history)\n    model.load_weights(filename_now)\n    model.save(filename_mlp)\n\nvalid_predict = model.predict(X_valid)\nprint(f\"TEST mape: {(mape(y_valid, valid_predict))*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Каггл","metadata":{}},{"cell_type":"code","source":"submission['price'] = model.predict(X_test)\nsubmission.to_csv('submission_nn_1.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# v2 ... + NLP","metadata":{}},{"cell_type":"markdown","source":"У меня получился хороший результат через emedding с использованием doc2vec (чуть лучше, чем word2vec с усреднением). Плюс само обучение происходит проще. Результат добавим к полносвязной нейронке из предыдущего варианта.","metadata":{}},{"cell_type":"markdown","source":"## Готовим embeddings","metadata":{}},{"cell_type":"code","source":"EMBEDDING_SIZE = 300","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# простая токенизация, т.к. текст уже подготовлен\ntkn_description = data['description'].str.split()\n\n# корпусы для doc2vec\nC_train = [doc2vec.TaggedDocument(tkn_description.iloc[i], [i]) for i in X_train.index]\nC_valid = [doc2vec.TaggedDocument(tkn_description.iloc[i], [i]) for i in X_valid.index]\nC_test = [tkn_description.iloc[i] for i in X_test.index]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# сама модель\nmdl_d2v = doc2vec.Doc2Vec(vector_size=EMBEDDING_SIZE,\n                          min_count=2,  # встречается хотя бы 2 раза\n                          seed=RANDOM_SEED,\n                         )\n# строим словарь\nmdl_d2v.build_vocab(C_train)\n# и учим\nmdl_d2v.train(C_train, \n              total_examples=mdl_d2v.corpus_count, \n              epochs=mdl_d2v.epochs\n             )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обработаем все тексты\nemb_description = tkn_description.map(mdl_d2v.infer_vector)\nemb_description.sample()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"V_train = np.array(emb_description.iloc[X_train.index].tolist())\nV_valid = np.array(emb_description.iloc[X_valid.index].tolist())\nV_test = np.array(emb_description.iloc[X_test.index].tolist())\nprint(V_train.shape)\nprint(V_valid.shape)\nprint(V_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Собираем multi-input NN","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 55\nnp.random.seed(RANDOM_SEED)\n\nfilename_nlp = '../working/best_nn_2_d2v.hdf5'\n\nLR = 1e-3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mdl_nlp = Sequential([\n    L.Dense(1024, input_dim=V_train.shape[1], activation=\"relu\"),\n    L.Dropout(0.25),\n    L.Dense(256, activation=\"relu\"),\n    L.Dropout(0.25),\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combinedInput = L.concatenate([mdl_nlp.output, mdl_mlp.output])\n\n# being our regression head\nhead = L.Dense(128, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\n\nmodel = Model(inputs=[mdl_nlp.input, mdl_mlp.input], outputs=head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Обучаем","metadata":{}},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(LR, amsgrad=False,)\n\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(filename_now,\n                             monitor='val_MAPE', \n                             verbose=1, \n                             mode='min',\n                             save_best_only = True,\n                            )\n\nearlystop = EarlyStopping(monitor='val_MAPE',\n                          patience=20,\n                          min_delta = 0.001,\n                          restore_best_weights=True,\n                          verbose=1,\n                         )\n\ncallbacks_list = [checkpoint, earlystop]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    model.load_weights(filename_nlp)\nexcept:\n    history = model.fit([V_train, X_train], y_train,\n                         batch_size=128,\n                         epochs=1000, # until EarlyStopping\n                         validation_data=([V_valid, X_valid], y_valid),\n                         callbacks=callbacks_list,\n                         verbose=0,\n                        )\n    plot_history(history)\n    model.load_weights(filename_now)\n    model.save(filename_nlp)\n\nvalid_predict = model.predict([V_valid, X_valid])\nprint(f\"TEST mape: {(mape(y_valid, valid_predict))*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Каггл","metadata":{}},{"cell_type":"code","source":"submission['price'] = model.predict([V_test, X_test])\nsubmission.to_csv('submission_nn_2.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# v3 ... + Images","metadata":{}},{"cell_type":"markdown","source":"### Data","metadata":{}},{"cell_type":"code","source":"# убедимся, что цены и фото подгрузились верно\nplt.figure(figsize = (12,8))\n\nrandom_image = X_train.head(9)\nrandom_image_paths = data.loc[random_image.index, 'sell_id'].values\nrandom_image_cat = y.loc[random_image.index, 'price'].values\n\nfor index, path in enumerate(random_image_paths):\n    im = PIL.Image.open('../input/sf-dst-car-price-prediction-part2/img/img/' + str(path) + '.jpg')\n    plt.subplot(3, 3, index + 1)\n    plt.imshow(im)\n    plt.title('price: ' + str(random_image_cat[index]))\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# у нас чистые изображения одного размера\nIMG_SIZE = (320, 240)\n\ndef get_image_array(index):\n    images = []\n    for index, sell_id in enumerate(data.loc[index, 'sell_id'].values):\n        image = cv2.imread('../input/sf-dst-car-price-prediction-part2/img/img/' + str(sell_id) + '.jpg')\n        assert(image is not None)\n        image = cv2.resize(image, IMG_SIZE)  # без этого возникают траблы\n        images.append(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) # перевёрнем цветовую схему, чтобы не бесило\n    images = np.array(images)\n    print('images shape', images.shape, 'dtype', images.dtype)\n    return(images)\n\n# все изображения в RAM, можем себе позволить\nI_train = get_image_array(X_train.index)\nI_valid = get_image_array(X_valid.index)\nI_test = get_image_array(X_test.index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# проверим правильные цвета\nplt.figure(figsize = (12,8))\nfor i in range(9):\n    img = I_train[i]\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Images in RAM: {(I_train.nbytes+I_valid.nbytes+I_test.nbytes)/1024**3:0.2f} GB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### albumentations","metadata":{}},{"cell_type":"code","source":"from albumentations import (\n    HorizontalFlip, \n    MultiplicativeNoise, GaussNoise, JpegCompression,\n    MotionBlur, MedianBlur, Blur,\n    ShiftScaleRotate,\n    OpticalDistortion, GridDistortion, ElasticTransform,\n    CLAHE, IAASharpen, IAAEmboss, RandomBrightnessContrast,\n    HueSaturationValue,\n    OneOf, Compose,\n)\n\n# документация здесь https://albumentations.ai/docs/getting_started/image_augmentation/\n# default p=0.5\naugmentation = Compose([\n    HorizontalFlip(),\n    OneOf([\n        MultiplicativeNoise(),\n        GaussNoise(),\n        JpegCompression(),\n    ]),\n    OneOf([\n        MotionBlur(),\n        MedianBlur(),\n        Blur(),\n    ]),\n    ShiftScaleRotate(rotate_limit=20),\n#     OneOf([\n#         OpticalDistortion(),\n#         GridDistortion(),\n#         ElasticTransform(),\n#     ]),\n    OneOf([\n        CLAHE(),\n        IAASharpen(),\n        IAAEmboss (),\n        RandomBrightnessContrast(),\n    ]),\n    HueSaturationValue(),\n], p=1)\n\n# пример\nplt.figure(figsize = (12,8))\nfor i in range(9):\n    img = augmentation(image = I_train[0])['image']\n    plt.subplot(3, 3, i + 1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def make_augmentations(images):\n#   print('применение аугментаций', end = '')\n#   augmented_images = np.empty(images.shape)\n#   for i in range(images.shape[0]):\n#     if i % 200 == 0:\n#       print('.', end = '')\n#     augment_dict = augmentation(image = images[i])\n#     augmented_image = augment_dict['image']\n#     augmented_images[i] = augmented_image\n#   print('')\n#   return augmented_images","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## tf.data.Dataset\nЕсли все изображения мы будем хранить в памяти, то может возникнуть проблема ее нехватки. Не храните все изображения в памяти целиком!\n\nМетод .fit() модели keras может принимать либо данные в виде массивов или тензоров, либо разного рода итераторы, из которых наиболее современным и гибким является [tf.data.Dataset](https://www.tensorflow.org/guide/data). Он представляет собой конвейер, то есть мы указываем, откуда берем данные и какую цепочку преобразований с ними выполняем. Далее мы будем работать с tf.data.Dataset.\n\nDataset хранит информацию о конечном или бесконечном наборе кортежей (tuple) с данными и может возвращать эти наборы по очереди. Например, данными могут быть пары (input, target) для обучения нейросети. С данными можно осуществлять преобразования, которые осуществляются по мере необходимости ([lazy evaluation](https://ru.wikipedia.org/wiki/%D0%9B%D0%B5%D0%BD%D0%B8%D0%B2%D1%8B%D0%B5_%D0%B2%D1%8B%D1%87%D0%B8%D1%81%D0%BB%D0%B5%D0%BD%D0%B8%D1%8F)).\n\n`tf.data.Dataset.from_tensor_slices(data)` - создает датасет из данных, которые представляют собой либо массив, либо кортеж из массивов. Деление осуществляется по первому индексу каждого массива. Например, если `data = (np.zeros((128, 256, 256)), np.zeros(128))`, то датасет будет содержать 128 элементов, каждый из которых содержит один массив 256x256 и одно число.\n\n`dataset2 = dataset1.map(func)` - применение функции к датасету; функция должна принимать столько аргументов, каков размер кортежа в датасете 1 и возвращать столько, сколько нужно иметь в датасете 2. Пусть, например, датасет содержит изображения и метки, а нам нужно создать датасет только из изображений, тогда мы напишем так: `dataset2 = dataset.map(lambda img, label: img)`.\n\n`dataset2 = dataset1.batch(8)` - группировка по батчам; если датасет 2 должен вернуть один элемент, то он берет из датасета 1 восемь элементов, склеивает их (нулевой индекс результата - номер элемента) и возвращает.\n\n`dataset.__iter__()` - превращение датасета в итератор, из которого можно получать элементы методом `.__next__()`. Итератор, в отличие от самого датасета, хранит позицию текущего элемента. Можно также перебирать датасет циклом for.\n\n`dataset2 = dataset1.repeat(X)` - датасет 2 будет повторять датасет 1 X раз.\n\nЕсли нам нужно взять из датасета 1000 элементов и использовать их как тестовые, а остальные как обучающие, то мы напишем так:\n\n`test_dataset = dataset.take(1000)\ntrain_dataset = dataset.skip(1000)`\n\nДатасет по сути неизменен: такие операции, как map, batch, repeat, take, skip никак не затрагивают оригинальный датасет. Если датасет хранит элементы [1, 2, 3], то выполнив 3 раза подряд функцию dataset.take(1) мы получим 3 новых датасета, каждый из которых вернет число 1. Если же мы выполним функцию dataset.skip(1), мы получим датасет, возвращающий числа [2, 3], но исходный датасет все равно будет возвращать [1, 2, 3] каждый раз, когда мы его перебираем.\n\ntf.Dataset всегда выполняется в graph-режиме (в противоположность eager-режиму), поэтому либо преобразования (`.map()`) должны содержать только tensorflow-функции, либо мы должны использовать tf.py_function в качестве обертки для функций, вызываемых в `.map()`. Подробнее можно прочитать [здесь](https://www.tensorflow.org/guide/data#applying_arbitrary_python_logic).","metadata":{}},{"cell_type":"code","source":"# NLP part\n# tokenize = Tokenizer(num_words=MAX_WORDS)\n# tokenize.fit_on_texts(data.description)\n\n# def tokenize_(descriptions):\n#   return sequence.pad_sequences(tokenize.texts_to_sequences(descriptions), maxlen = MAX_SEQUENCE_LENGTH)\n\n# def tokenize_text(text):\n#     return tokenize_([text.numpy().decode('utf-8')])[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_image(image):\n    return augmentation(image = image.numpy())['image']\n\ndef tf_process_train_dataset_element(image, table_data, text_emb, price):\n    im_shape = image.shape\n    [image,] = tf.py_function(process_image, [image], [tf.uint8])\n    image.set_shape(im_shape)\n#     [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text_emb), price\n\ndef tf_process_val_dataset_element(image, table_data, text_emb, price):\n#     [text,] = tf.py_function(tokenize_text, [text], [tf.int32])\n    return (image, table_data, text_emb), price\n\nD_train = tf.data.Dataset.from_tensor_slices((\n    I_train, \n    X_train, \n    V_train, \n    y_train,\n    )).map(tf_process_train_dataset_element)\n\nD_valid = tf.data.Dataset.from_tensor_slices((\n    I_valid,\n    X_valid,\n    V_valid,\n    y_valid,\n    )).map(tf_process_val_dataset_element)\n\ny_test = np.zeros(len(X_test))\n\nD_test = tf.data.Dataset.from_tensor_slices((\n    I_test, \n    X_test,\n    V_test,\n    y_test,\n    )).map(tf_process_val_dataset_element)\n\n#проверяем, что нет ошибок (не будет выброшено исключение):\nnext(iter(D_train));\nnext(iter(D_valid));\nnext(iter(D_test));","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Собираем модель","metadata":{}},{"cell_type":"code","source":"RANDOM_SEED = 16\nnp.random.seed(RANDOM_SEED)\n\nfilename_cnn = '../working/best_nn_3_cnn.hdf5'\n\nLR = 1e-2  # используем callback для управления уменьшением\nBATCH_SIZE = 30","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# удалим файл модели, если хотим обучить заново\n# os.remove(filename_cnn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# efficientnet_model = tf.keras.applications.efficientnet.EfficientNetB3(weights = 'imagenet', include_top = False, input_shape = (size[1], size[0], 3))\n# efficientnet_output = L.GlobalAveragePooling2D()(efficientnet_model.output)\n\nmdl_cnn = EfficientNetB3(\n    input_shape = (IMG_SIZE[1], IMG_SIZE[0], 3),\n    include_top = False,\n    weights = 'imagenet',\n    pooling = 'avg',    # avg - max - None\n    classifier_activation = 'softmax',    # softmax - None\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# объединяем выходы трех нейросетей\ncombinedInput = L.concatenate([mdl_cnn.output, mdl_mlp.output, mdl_nlp.output])\n\n# being our regression head\nhead = L.Dense(256, activation=\"relu\")(combinedInput)\nhead = L.Dense(1,)(head)\n\nmodel = Model(inputs=[mdl_cnn.input, mdl_mlp.input, mdl_nlp.input], outputs=head)\n\n# model.summary()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(LR, amsgrad=False,)\n\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint = ModelCheckpoint(filename_now,\n                             monitor='val_MAPE', \n                             verbose=1, \n                             mode='min',\n                             save_best_only = True,\n                            )\n\nearlystop = EarlyStopping(monitor='val_MAPE',\n                          patience=25,\n                          min_delta = 0.001,\n                          restore_best_weights=True,\n                          verbose=1,\n                         )\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_MAPE',\n    factor=0.5,\n    patience=5,\n    verbose=1,\n    mode='min',\n    min_delta=0.001,\n    cooldown=0,\n    min_lr=0,\n)\n\ncallbacks_list = [checkpoint, earlystop, reduce_lr]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    model.load_weights(filename_cnn)\nexcept:\n    history = model.fit(D_train.batch(BATCH_SIZE),\n#                          batch_size=128,\n                         epochs=1000, # until EarlyStopping\n                         validation_data=D_valid.batch(BATCH_SIZE),\n                         callbacks=callbacks_list,\n                         verbose=0,\n                        )\n    plot_history(history)\n    model.load_weights(filename_now)\n    model.save(filename_cnn)\n\nvalid_predict = model.predict(D_valid.batch(BATCH_SIZE))\nprint(f\"TEST mape: {(mape(y_valid, valid_predict))*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Кагл","metadata":{}},{"cell_type":"code","source":"submission['price'] = model.predict(D_test.batch(BATCH_SIZE))\nsubmission.to_csv('submission_nn_3.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history = model.fit(D_train.batch(30),\n#                     epochs=100,\n#                     validation_data = D_valid.batch(30),\n#                     callbacks=callbacks_list\n#                    )","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.title('Loss')\n# plt.plot(history.history['MAPE'], label='train')\n# plt.plot(history.history['val_MAPE'], label='test')\n# plt.show();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.load_weights('../working/best_model.hdf5')\n# model.save('../working/nn_final.hdf5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_predict_nn3 = model.predict(D_valid.batch(30))\n# print(f\"TEST mape: {(mape(y_test, test_predict_nn3[:,0]))*100:0.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub_predict_nn3 = model.predict(sub_dataset.batch(30))\n# sample_submission['price'] = sub_predict_nn3[:,0]\n# sample_submission.to_csv('nn3_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n#### Общие рекомендации:\n* Попробовать разные архитектуры\n* Провести более детальный анализ результатов\n* Попробовать различные подходы в управление LR и оптимизаторы\n* Поработать с таргетом\n* Использовать Fine-tuning\n\n#### Tabular\n* В нейросеть желательно подавать данные с распределением, близким к нормальному, поэтому от некоторых числовых признаков имеет смысл взять логарифм перед нормализацией. Пример:\n`modelDateNorm = np.log(2020 - data['modelDate'])`\nСтатья по теме: https://habr.com/ru/company/ods/blog/325422\n\n* Извлечение числовых значений из текста:\nПарсинг признаков 'engineDisplacement', 'enginePower', 'Владение' для извлечения числовых значений.\n\n* Cокращение размерности категориальных признаков\nПризнак name 'name' содержит данные, которые уже есть в других столбцах ('enginePower', 'engineDisplacement', 'vehicleTransmission'). Можно удалить эти данные. Затем можно еще сильнее сократить размерность, например выделив наличие xDrive в качестве отдельного признака.\n\n* Поработать над Feature engineering\n\n\n\n#### NLP\n* Выделить из описаний часто встречающиеся блоки текста, заменив их на кодовые слова или удалив\n* Сделать предобработку текста, например сделать лемматизацию - алгоритм ставящий все слова в форму по умолчанию (глаголы в инфинитив и т. д.), чтобы токенайзер не преобразовывал разные формы слова в разные числа\nСтатья по теме: https://habr.com/ru/company/Voximplant/blog/446738/\n* Поработать над алгоритмами очистки и аугментации текста\n\n\n\n#### CV\n* Попробовать различные аугментации\n* Fine-tuning","metadata":{}},{"cell_type":"markdown","source":"# Blend","metadata":{}},{"cell_type":"code","source":"blend_predict = (test_predict_catboost + test_predict_nn3[:,0]) / 2\nprint(f\"TEST mape: {(mape(y_test, blend_predict))*100:0.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"blend_sub_predict = (sub_predict_catboost + sub_predict_nn3[:,0]) / 2\nsample_submission['price'] = blend_sub_predict\nsample_submission.to_csv('blend_submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Bonus: проброс признака","metadata":{}},{"cell_type":"code","source":"# MLP\nmodel_mlp = Sequential()\nmodel_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))\nmodel_mlp.add(L.Dense(256, activation=\"relu\"))\nmodel_mlp.add(L.Dropout(0.5))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# FEATURE Input\n# Iput\nproductiondate = L.Input(shape=[1], name=\"productiondate\")\n# Embeddings layers\nemb_productiondate = L.Embedding(len(X.productionDate.unique().tolist())+1, 20)(productiondate)\nf_productiondate = L.Flatten()(emb_productiondate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combinedInput = L.concatenate([model_mlp.output, f_productiondate,])\n# being our regression head\nhead = L.Dense(64, activation=\"relu\")(combinedInput)\nhead = L.Dense(1, activation=\"linear\")(head)\n\nmodel = Model(inputs=[model_mlp.input, productiondate], outputs=head)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = tf.keras.optimizers.Adam(0.01)\nmodel.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit([X_train, X_train.productionDate.values], y_train,\n                    batch_size=512,\n                    epochs=500, # фактически мы обучаем пока EarlyStopping не остановит обучение\n                    validation_data=([X_test, X_test.productionDate.values], y_test),\n                    callbacks=callbacks_list\n                   )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.load_weights('../working/best_model.hdf5')\ntest_predict_nn_bonus = model.predict([X_test, X_test.productionDate.values])\nprint(f\"TEST mape: {(mape(y_test, test_predict_nn_bonus[:,0]))*100:0.2f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}