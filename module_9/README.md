# skillfactory_rds / module_9

Название проекта: Car Price (part 2).

Задача проекта: Предсказать стоимость автомобиля с побегом, используя данные разной природы.

План действий по проекту:
+ Изучить данные
+ Провести EDA, обработать текст для NLP-нейронки
+ Обучить базовую модель, чтобы было с чем сравнить
+ Обучить градиентный бустинг по табличным данным (CatBoost или Extra-Trees)
+ Обучить классическую нейронку
+ Обучить multi-input нейронку (классика+NLP)
+ Обучить multi-input нейронку (классика+NLP+изображения)
+ Попробовать блендинг нейронки и градиентного бустинга

Как устроен проект:
- Изучение данных и предобработка (01-eda-and-preprocess-v2.ipynb), базовая модель (11-ML-simple.ipynb) и градиентный бустинг (12-ML-CatBoost-v2.ipynb) создавались в Jupyter на локальной машине.
- Нейронки (sf-dst-car-price-part2.ipynb) создавались и обучались на Kaggle, т.к. для экспериментов нужен GPU.
- Данные после предобработки и предсказание CatBoost (без округления) пробрасываются в Kaggle через подключаемый датасет, просто чтобы не тратить время на повторное вычисление.

Выводы по работе (применительно к данной конкретной задаче):
- Лучший по соотношению время/результат - CatBoost.
- Нейронка MLP+NLP тоже даёт адекватный результат, схожей точности. Но требует больше времени на эксперименты.
- Обработка NLP через эмбединги быстрее, чем подход с LSTM. Нормально считалось бы даже без GPU.
- Обработка изображений чудовищно дорога по вычислительным мощностям и времени на поиск. И не даёт прибавки.
- Блендинг CatBoost и MLP+NLP внезапно дал отличный прирост. Видимо произошла компенсация разнонаправленных предсказаний.
- Лучше потратить больше времени на обработку данных и feature engineering (с последовательной проверкой на том же CatBoost), чем сразу лезть в тяжёлые решения.

Ссылка на GitHub:
https://github.com/sau114/skillfactory_rds/tree/main/module_9

Ссылка на Kaggle:
https://www.kaggle.com/code/stepanopushnev/sf-dst-car-price-part2

Логин и результат на Kaggle: 
Stepan Opushnev - 10.80160
